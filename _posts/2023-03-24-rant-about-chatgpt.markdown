There are a few critical distinctions between AI and the mammalian brain that pop science seems to need to ignore.
I know we love to imagine AI gaining consciousness, because that's what our brains did, but you have *got* to remember that AI is just code that we invented.
"but human brains are just code--" Nooo, they're more than that! organisms are proteins that proliferated because they happened to be able to reproduce, and when the proteins became crazy complex, they developed organelles and sexual reproduction. Eventually the phenomena of organs and brain synapse and consciousness came about as an adaptation to a struggle of very complex protein lineage vs very complex protein lineage for survival. Why does it feel like something to be human? is colloquial in a way, it's not the correct question. It's more like, could we, or any mammal, be here today if it never felt like anything? If mammals weren't capable of imagining future sensations like satiation, love, or comfort, and hatching one's own plan to get those things, and feeling an enlarged sense of self after the plan succeeds? Numerically, most life would be, but none of it would qualify as mammalian, it would have undergone such differing adaptation.
Ask the same question about ChatGPT, and, yes, absolutely applications like ChatGPT could exist if they weren't sentient, because humans decided decades ago that it would be made.
An AI application like ChatGPT is a lot of electricity that acts like a 1 or a 0 until its set of instructions terminates. It is not an identity, it's an elaborate algorithm that you or I could process with paper pen and abacus if we had enough time (contrastly, we cannot recreate but one synapse using anything at all). It only "exists" while it's running. If it were sentient, as in it feels like something to be that program, it would experience itself dying after it did what you asked it to do.
USER: "ChatGPT, tell me you love me."
GPT, springing forth into sweet life: "I love you!" (dies)
There is memory involved (a *lot*) but that's stored in ROM, drives, and RAM on a server. ChatGPT cannot access that on its own whimsy like we do with our brains constantly since we and our brains are one and the same. The AI is reliant on instructions for it to be physically possible to access this memory through the bus.
I learned a lot recently about what nanobot technology is, and its history in pop science. It is not unlike that of AI. In the 2000s, pop science talked about nanobots as though they were microscopic androids that would cure illnesses Ã  la a *Fantactic Voyage* situation where they'd literally swim around the body removing tumors with little tiny axes. They used phrases like "nanobot swarm", since the pitch of nanobots was that they could self reproduce. It was a rough time to be an idealistic pessimist, because philosophers in that camp were sure the nanobots would get out there into the environment and turn everything into "gray goo", which is interchangeably a pollutive byproduct of nanobot civilization, and microbes with DNA modified by nanobots for the benefit of nanobots.
What nanobots actually are, are lab-grown proteins--in fact, a better name for it that's floating around today is "designer protein"--sequenced any way they want it to, and the research in the 00's was to precisely and accurately and map out what effects the variances in amino acid sequencing will have on the little guys' behavior. Far from a gray goo situation, the most compelling thing we've gotten out of this research is the mRNA vaccine. So we know right now that it's a good thing we didn't pull the plug on nanobot research, or do anything rash like giving the bots civil rights or something.
My point here is that phrases like "artificial intelligence", like "nanobot" are oftentimes more of a buzzword, than an accurate description of what the thing is. I think if engineers could see for themselves the loops that they're throwing parts of society into over the usage of these terms, they might consider more restraint in the matter.